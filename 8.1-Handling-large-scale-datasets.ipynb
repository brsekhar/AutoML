{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8.1-Handling-large-scale-datasets",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aX1dRHHVUUs"
      },
      "source": [
        "!pip install -r https://raw.githubusercontent.com/datamllab/automl-in-action-notebooks/master/requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7PWbzmdsTZ3"
      },
      "source": [
        "## 8.1.1 Loading image classification dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQLw4tctMD36",
        "outputId": "160aecb1-761c-4c72-cb7a-ffd4095c5000"
      },
      "source": [
        "!wget https://github.com/datamllab/automl-in-action-notebooks/raw/master/data/mnist.tar.gz\n",
        "!tar xzf mnist.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-25 20:09:24--  https://github.com/datamllab/automl-in-action-notebooks/raw/master/data/mnist.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/datamllab/automl-in-action-notebooks/master/data/mnist.tar.gz [following]\n",
            "--2021-07-25 20:09:24--  https://raw.githubusercontent.com/datamllab/automl-in-action-notebooks/master/data/mnist.tar.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17472747 (17M) [application/octet-stream]\n",
            "Saving to: ‘mnist.tar.gz.1’\n",
            "\n",
            "mnist.tar.gz.1      100%[===================>]  16.66M  84.3MB/s    in 0.2s    \n",
            "\n",
            "2021-07-25 20:09:25 (84.3 MB/s) - ‘mnist.tar.gz.1’ saved [17472747/17472747]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irnT7kbEH8zA"
      },
      "source": [
        "```\n",
        "train/\n",
        "  0/\n",
        "    1.png\n",
        "    21.png\n",
        "    ...\n",
        "  1/\n",
        "  2/\n",
        "  3/\n",
        "  ...\n",
        "\n",
        "test/\n",
        "  0/\n",
        "  1/\n",
        "  ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_N3m_ublsTZ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3abd9bc2-edfc-438f-bf3f-a008d99a5915"
      },
      "source": [
        "import os\n",
        "import autokeras as ak\n",
        "\n",
        "batch_size = 32\n",
        "img_height = 28\n",
        "img_width = 28\n",
        "\n",
        "parent_dir = 'data'\n",
        "\n",
        "test_data = ak.image_dataset_from_directory(\n",
        "    os.path.join(parent_dir, 'test'),\n",
        "    seed=123,\n",
        "    color_mode=\"grayscale\",\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "for images, labels in test_data.take(1):\n",
        "    print(images.shape, images.dtype)\n",
        "    print(labels.shape, labels.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10000 files belonging to 10 classes.\n",
            "(32, 28, 28, 1) <dtype: 'float32'>\n",
            "(32,) <dtype: 'string'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4k6bzo-LK2X"
      },
      "source": [
        "## 8.1.2 Splitting the loaded dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQs1QVgNjAiF",
        "outputId": "05e9bc98-44df-46ed-bfc9-f0b996d9ec2a"
      },
      "source": [
        "all_train_data = ak.image_dataset_from_directory(\n",
        "    os.path.join(parent_dir, 'train'),\n",
        "    seed=123,\n",
        "    color_mode=\"grayscale\",\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "train_data = all_train_data.take(int(60000 / batch_size * 0.8))\n",
        "validation_data = all_train_data.skip(int(60000 / batch_size * 0.8))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 60000 files belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IoxdxBVVS4v",
        "outputId": "b4218668-6a48-4fa9-d98e-f9187aedc0e4"
      },
      "source": [
        "train_data = ak.image_dataset_from_directory(\n",
        "    os.path.join(parent_dir, 'train'),\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    color_mode=\"grayscale\",\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "validation_data = ak.image_dataset_from_directory(\n",
        "    os.path.join(parent_dir, 'train'),\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    color_mode=\"grayscale\",\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 60000 files belonging to 10 classes.\n",
            "Using 48000 files for training.\n",
            "Found 60000 files belonging to 10 classes.\n",
            "Using 12000 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utYsWdI9yHpt"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "train_data = train_data.prefetch(5)\n",
        "validation_data = validation_data.prefetch(5)\n",
        "test_data = test_data.prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWnnl5ZysTZ5"
      },
      "source": [
        "Then we just do one quick demo of AutoKeras to make sure the dataset works.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qoadht1XsTZ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "247ae09f-7e25-4144-bcd4-58ddcaa5fd95"
      },
      "source": [
        "clf = ak.ImageClassifier(overwrite=True, max_trials=1)\n",
        "clf.fit(train_data, epochs=1, validation_data=validation_data)\n",
        "print(clf.evaluate(test_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 1 Complete [00h 03m 44s]\n",
            "val_loss: 0.06113607808947563\n",
            "\n",
            "Best val_loss So Far: 0.06113607808947563\n",
            "Total elapsed time: 00h 03m 44s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "1500/1500 [==============================] - 200s 133ms/step - loss: 0.1763 - accuracy: 0.9463 - val_loss: 0.0626 - val_accuracy: 0.9829\n",
            "INFO:tensorflow:Assets written to: ./image_classifier/best_model/assets\n",
            "313/313 [==============================] - 27s 83ms/step - loss: 0.0508 - accuracy: 0.9834\n",
            "[0.05080397054553032, 0.9833999872207642]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSUxx-N3sTZ6"
      },
      "source": [
        "## 8.1.3 Loading text classification dataset\n",
        "You can also load text datasets in the same way.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryaY6xGAx2Gd",
        "outputId": "65d1fe6c-d551-4a06-927c-4af8e52f910c"
      },
      "source": [
        "!wget https://github.com/datamllab/automl-in-action-notebooks/raw/master/data/imdb.tar.gz\n",
        "!tar xzf imdb.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-25 20:18:35--  https://github.com/datamllab/automl-in-action-notebooks/raw/master/data/imdb.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/datamllab/automl-in-action-notebooks/master/data/imdb.tar.gz [following]\n",
            "--2021-07-25 20:18:35--  https://raw.githubusercontent.com/datamllab/automl-in-action-notebooks/master/data/imdb.tar.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29215039 (28M) [application/octet-stream]\n",
            "Saving to: ‘imdb.tar.gz.1’\n",
            "\n",
            "imdb.tar.gz.1       100%[===================>]  27.86M  62.4MB/s    in 0.4s    \n",
            "\n",
            "2021-07-25 20:18:35 (62.4 MB/s) - ‘imdb.tar.gz.1’ saved [29215039/29215039]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_69yOoAsTZ7"
      },
      "source": [
        "For this dataset, the data is already split into train and test.\n",
        "We just load them separately.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj4K8dcTsTZ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6267dc03-c625-4aec-9ecf-59598186ce37"
      },
      "source": [
        "import os\n",
        "import autokeras as ak\n",
        "import tensorflow as tf\n",
        "\n",
        "train_data = ak.text_dataset_from_directory(\n",
        "    \"imdb/train\",\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    max_length=1000,\n",
        "    batch_size=32,\n",
        ").prefetch(1000)\n",
        "\n",
        "validation_data = ak.text_dataset_from_directory(\n",
        "    \"imdb/train\",\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    max_length=1000,\n",
        "    batch_size=32,\n",
        ").prefetch(1000)\n",
        "\n",
        "test_data = ak.text_dataset_from_directory(\n",
        "    \"imdb/test\",\n",
        "    max_length=1000,\n",
        ").prefetch(1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tgDzl9s1wIB",
        "outputId": "5ca9e88b-1d19-48dd-c628-f909727555f2"
      },
      "source": [
        "clf = ak.TextClassifier(overwrite=True, max_trials=1)\n",
        "clf.fit(train_data, epochs=2, validation_data=validation_data)\n",
        "print(clf.evaluate(test_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 1 Complete [00h 05m 40s]\n",
            "val_loss: 0.33729812502861023\n",
            "\n",
            "Best val_loss So Far: 0.33729812502861023\n",
            "Total elapsed time: 00h 05m 40s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "Epoch 1/2\n",
            "625/625 [==============================] - 160s 254ms/step - loss: 0.5043 - accuracy: 0.7269 - val_loss: 0.3593 - val_accuracy: 0.8386\n",
            "Epoch 2/2\n",
            "625/625 [==============================] - 157s 251ms/step - loss: 0.3008 - accuracy: 0.8734 - val_loss: 0.3541 - val_accuracy: 0.8424\n",
            "INFO:tensorflow:Assets written to: ./text_classifier/best_model/assets\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 0.3539 - accuracy: 0.8450\n",
            "[0.35387808084487915, 0.8450400233268738]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwhWMAQCLnci"
      },
      "source": [
        "## 8.1.4 Handling large dataset in general format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YF4JhrBZp5Tc",
        "outputId": "e637bd94-373c-4a2d-d2f9-e0eab458b805"
      },
      "source": [
        "data = [5, 8, 9, 3, 6]\n",
        "def generator():   \n",
        "    for i in data: \n",
        "        yield i   \n",
        "          \n",
        "for x in generator():\n",
        "    print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "8\n",
            "9\n",
            "3\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vB4fgU0Atwa4",
        "outputId": "5ca907df-93a1-49be-e796-e06556400154"
      },
      "source": [
        "dataset = tf.data.Dataset.from_generator(\n",
        "    generator,\n",
        "    output_types=tf.int32)\n",
        "for x in dataset:\n",
        "    print(x.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "8\n",
            "9\n",
            "3\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AATInn4xZfXg",
        "outputId": "24d7d23e-2cbe-4956-9f5b-3e99ace61d96"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "path = os.path.join(parent_dir, \"train\")\n",
        "\n",
        "def load_data(path):\n",
        "    data = []\n",
        "    for class_label in [\"pos\", \"neg\"]:\n",
        "        for file_name in os.listdir(os.path.join(path, class_label)):\n",
        "            data.append((os.path.join(path, class_label, file_name), class_label))\n",
        "\n",
        "    data = np.array(data)\n",
        "    np.random.shuffle(data)\n",
        "    return data\n",
        "\n",
        "def get_generator(data):\n",
        "    def data_generator():\n",
        "        for file_path, class_label in data:\n",
        "            text_file = open(file_path, \"r\")\n",
        "            text = text_file.read()\n",
        "            text_file.close()\n",
        "            yield text, class_label\n",
        "    return data_generator\n",
        "    \n",
        "all_train_np = load_data(os.path.join(parent_dir, \"train\"))\n",
        "\n",
        "def np_to_dataset(data_np):\n",
        "  return tf.data.Dataset.from_generator(\n",
        "    get_generator(data_np), \n",
        "    output_types=tf.string,\n",
        "    output_shapes=tf.TensorShape([2]),\n",
        "  ).map(lambda x: (x[0], x[1])).batch(32).prefetch(5)\n",
        "\n",
        "train_data = np_to_dataset(all_train_np[:20000])\n",
        "validation_data = np_to_dataset(all_train_np[20000:])\n",
        "test_np = load_data(os.path.join(parent_dir, \"test\"))\n",
        "test_data = np_to_dataset(test_np)\n",
        "\n",
        "for texts, labels in train_data.take(1):\n",
        "    print(texts.shape)\n",
        "    print(labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32,)\n",
            "(32,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQyp1Ohfg2iL",
        "outputId": "19a91a71-39ce-4c91-8a49-20bb7548ec02"
      },
      "source": [
        "clf = ak.TextClassifier(overwrite=True, max_trials=1)\n",
        "clf.fit(train_data, epochs=2, validation_data=validation_data)\n",
        "print(clf.evaluate(test_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 1 Complete [00h 07m 08s]\n",
            "val_loss: 0.2818661630153656\n",
            "\n",
            "Best val_loss So Far: 0.2818661630153656\n",
            "Total elapsed time: 00h 07m 08s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "Epoch 1/2\n",
            "625/625 [==============================] - 165s 262ms/step - loss: 0.4648 - accuracy: 0.7589 - val_loss: 0.3007 - val_accuracy: 0.8780\n",
            "Epoch 2/2\n",
            "625/625 [==============================] - 154s 245ms/step - loss: 0.2484 - accuracy: 0.8974 - val_loss: 0.2964 - val_accuracy: 0.8802\n",
            "INFO:tensorflow:Assets written to: ./text_classifier/best_model/assets\n",
            "782/782 [==============================] - 61s 78ms/step - loss: 0.2827 - accuracy: 0.8845\n",
            "[0.282741516828537, 0.8845199942588806]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
